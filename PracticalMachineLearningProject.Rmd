---
title: "PracticalMachineLearningProject"
author: "Jody P. Abney"
date: "August 26, 2014"
output: html_document
---

## Executive Summary
**Reword** Actually, people love to record all the training they do, using their smartphones. So there are a lot of useful data that can provide very smart information about the way they are doing exercises.

The goal of this study is to predict the manner in which people do their exercises. Based on a set of records we are going to study the way they are training by developing a model using the Random Forest approach. The dataset contains 160 variables. One of them, called "classe" is the target to predict and the others 159 are going to be analysed in order to determine whether or not they are useful for our model purpose (predictors).


## Data Analysis Approach
```{r Setup}
# Load required libraries
library(caret) # Load caret package
library(randomForest) # Load randomForest package
```

We have two data sets located in ./data sub directory: 

* "pml-training.csv" (training data set)
* "pml.testing.csv" (testing data set)

```{r}
# Read training and testing data sets
training <- read.csv("./data/pml-training.csv", sep=",", header=TRUE)
testing <- read.csv("./data/pml-testing.csv", sep=",", header=TRUE)

dim(training)
```

The target variable (classe) is a categorical variable and has five different values (A, B, C, D, E) denoting how the participants performed one of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). (For more details on the study background and data collection, see http://groupware.les.inf.puc-rio.br/har).

### Training Set Variable Analysis and Cleansing
Given the large width (total variables = 160) in the data set, we can work to reduce the data set to a more manageable collection without impacting the robustness of the data themselves. By analyzing the columns in the training data set, we determined the following columns could be removed from our model based on a `summary(training)` function:

* Variable "X" (column 1) appears to be a somple rowstamp that provides no value for model prediction
* Variable "user-name" (column 2) simply captures the inidividual subject performing the activities and provides no value for model prediction
* Variable "raw_timestamp_part_1" (column 3) is a timestamp value and provides no value for model prediction
* Variable "raw_timestamp_part_2" (column 4) is a timestamp value and provides no value for model prediction
* Variable "cvtd_timestamp" (column 5) is a datetime value and provides no value for model prediction
* Variable "new_window" (column 6) is heavily skewed toward "no" values (~97.9%) and provides little value for model prediction

```{r, echo=TRUE}
training <- training[, -c(1, 2, 3, 4, 5, 6)]
```

A large presence of "NA" values were also observed in the `summary(training)` results. Given the prevelance of the "NA" values, we devise an approach to ignore (remove) the variables with "NA" value populations greater than the "benchmark/cutoff." We arbitarily choose a "benchmark/cutoff" of 20% (i.e. variables having an "NA" value population greater than 20% will not be included in the model).

```{r echo=TRUE, warning=FALSE}

cols <- ncol(training) # Get the number of columns
rows <- nrow(training) # Get the number of rows

# Define an array with the column ids to delete from the training data set
delete_col <- c()

# Calculate the number of NA values for each column except "classe" column (our 
# outcome variable which is the last column in the data set)
for( i in 1:cols-1){
    
    # Get number of NA values for the current column
    tmp <- nrow(training[is.na(as.numeric(as.character(training[, i]))), ])
    
    # Apply our arbtary benchmark/cutoff of 20% NA population in  a variable
    # based on the assumption that such as vairable would result in that 
    # variable being a bad predictor input for the model. Log the column
    # number in the "delete_col" array.
    if(tmp > round(0.20 * rows, 0)){
        delete_col <- c(delete_col, i)
        }   
    }

# Create a new datset without all the columns that have more than 
# 20% population of NA values
training <- training[, -delete_col]

dim(training)
```

At this point we have reduced our training data set removing unnecessary variables resulting in a remaining variable set of 54.

### Principal Components Analysis (PCA)

We next find the correlated predictors using doing a principal components analysis (PCA) on the training data set.

```{r echo=TRUE}

# Create a correlated matrix with all variables except our "classe" (outcome) 
# variable
correlated <- abs(cor(training[, -54]))
diag(correlated) <- 0 # All variables have a correlation of 1 with themselves
correlated <- which(correlated > 0.8, arr.ind=T)

length(unique(sort(correlated)))

```

Based on the correlation matrix, there are correlated variables (22). Our model, based on a Random Forest approach, will decide what are the best predictors for our target, so it is not necessary to make combinations of the initial variables.

### Cross-Validation Application
Using Cross-Validation techniques and the large training data set population (~20,000), we sampled our training data set in two new data sets for testing our Random Forest model approach. We established a new training data set (newtrain) and a new test data set (newtest) using a 20% sampling from the cleaned training data set.

```{r}

# Get training sample (20% of the total training dataset size)
inTrain <- createDataPartition(y = training$classe, p=0.2, list=FALSE)
newtrain <- training[inTrain, ]
newtest <- training[-inTrain, ]

```

### Random Forest Approach

```{r echo=TRUE}

# Plot our categorical outcome variable (classe). Can't use a historgram 
# because the variable isn't numeric
qplot(classe, data=training3)

# Set seed to repeat the test
set.seed(999)

# Fit the model for our outomce vairable (class) using all 53 predictors 
# using a Random Forest (rf) method
modelFit <- train(classe ~ ., data=newtrain, method="rf", prox=TRUE)

# optimal model with mtry = 27
print(modelFit)

# See final model
print(modelFit$finalModel)

# Get the specific selected tree (27)
selectedTree <- getTree(modelFit$finalModel, k=27)

```

The OOB error rate is 1.63%.

### Test Model with newtest Data Set

We next used the newtest data set (sampled from our our cleaned training data set to test the model.

```{r echo=TRUE}

predictions <- predict(modelFit, newtest)

confusionMatrix(predictions, newtest$classe)

# Print predictions
qplot(predictions, classe, data=newtest)

```

Based on predictions from our newtest dataset, we observe an Accuracy of 0.982 (98%) confirming our model approach is good to predict our outome variable (classe)

